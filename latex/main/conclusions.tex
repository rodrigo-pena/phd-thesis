\chapter{Conclusions}

\quote[0.625\textwidth]{I want to stand as close to the edge as I can without going over. Out on the edge you see all kinds of things you can't see from the center.}{Kurt Vonnegut, Player Piano}

To find the missing values, we first connected the dots. Discrete signals are often informed by the similarity of its support points. Groups of friends tend to watch the same movies; politicians that vote alike tend to be in the same party. Those similarities can be concisely represented through a network, and the original signal becomes a graph signal. Still, I have shown in the beginning of this thesis how to express the sampling and analysis of graph signals in the familiar language of linear algebra. The graph gradient matrix, $\mathbf{D}$, arose as the analysis operator for piecewise-constant signals, that is, those that have few edge-variations. Matrix $\mathbf{A}$, representing vertex-sampling, was formed by stacking independent random rows from the standard basis in $\mathbb{R}^{n}$, according to two alternative sampling models --- \acrshort{ber} and \acrshort{cswr}.

To figure the whole from its samples, we exploited a signature feature. For graph signals that are piecewise-constant, a good potential signature is the count of edges across which the signal varies. Among all possible ways to ``count'' the edge differences, I contrasted the \acrfull{gtv} and the Dirichlet form. Both theoretically and numerically, I argued that \acrshort{gtv} minimization is less sensitive to the sampling \emph{operator}. In particular, its recovery error drops suddenly close to zero as long as \emph{the number} of measurements is large enough.

To reveal the best sampling design, we minimized the sample complexity. I explored two approaches to study the recovery conditions under \acrshort{gtv} minimization. The ``direct'' path of Chapter \ref{ch:lower_bound_min_gain} is the more general --- dealing automatically with noisy samples --- but I was forced to leave it without a punchline. What blocked it was the lack of knowledge on the coordinate structure of the decent cone $\mathcal{D} \left ( \|\mathbf{D} \cdot\|_1 , \mathbf{x} \right )$. The ``dual'' path of Chapter \ref{ch:inexact_dual} was more fruitful, revealing the number of measurements required for exact \acrshort{gtv} interpolation as a function of the sampling probabilities $\bm{\pi} =(\pi_1, \dots, \pi_n)$. What followed was an explicit expression for the optimal vector $\bm{\pi}$.

To realize the practical issues, we finally took a numerical tour. Implementing the decoders was fairly easy; implementing the optimal sampling design is less so. Nevertheless, we saw how an approximation of the optimal design can visibly change the phase transition profile of the recovery error in \acrshort{gtv} interpolation. But this approximate design has to depend on the ground-truth signal's jump-set, knowledge of which is unobtainable a priori. Indeed --- for all the different datasets of Chapter~\ref{ch:numerical_tour} ---, the \emph{naive coherence design} behaved just as well as \emph{uniform random sampling}, while the \emph{jump-set coherence} alternative moved the sample complexity threshold towards lower measurement levels.

I leave the readers with two lists that summarize what we have learned in this thesis and what we still have to learn about sampling and subsequent \acrshort{gtv} recovery of piecewise-constant graph signals.

\section{Takeaways}

\begin{itemize}
    \item \textbf{Piecewise-constant signals and the geometry of $\mathbb{B}_{\acrshort{gtv}}$.} The extreme ``points'' of the \acrfull{gtv} ball are essentially indicator vectors of vertex subsets. These indicators are basic elements for a sparse description of piecewise-constant graph functions. On the one hand, the solutions of \acrshort{gtv} minimization will thus tend to be sparse combinations of vertex-subset indicator vectors. On the other hand, the extreme points of the solution set can be \emph{represented} by a selection of columns from the pseudo-inverse $\mathbf{D}^{+}$. Piecewise-constant graph signals that are ``compatible'' with the graph structure require relatively few atoms from $\mathbf{D}^{+}$ in its description. Compatible signals can be potentially recovered by \acrshort{gtv} minimization using only a few vertex samples.
    \item \textbf{Optimal sampling design for \acrshort{gtv} interpolation.} The sampling probabilities that minimize the recovery threshold for \acrshort{gtv} interpolation depend on vertex-induced perturbations of the projection operator $\mathbf{D D}^{+}$, restricted to the jump-set $\mathcal{S} := \operatorname{supp}\left ( \mathbf{Dx} \right )$ of the signal $\mathbf{x}$ to be recovered.
    \item \textbf{Importance of the jump-set in the sample complexity threshold.} Attempting to write the sample complexity of \acrshort{gtv} decoders as simply proportional to the jump-set's cardinality might incur in vacuous bounds. This is true, first, because $|\mathcal{S}|$ in piecewise-constant graph signals can be much larger than the signal's dimension and, yet, these signals can still be successfully recovered from relatively few measurements. Second, a sampling design based on this \emph{naive} control over the jump-set induces the same recovery error profile as if the vertices were sampled uniformly at random.
\end{itemize}

\section{Open problems}

\begin{itemize}
    \item \textbf{A direct certificate for \acrshort{gtv} recovery.} Knowing more about the coordinate structure of the descent cone $\mathcal{D} \left ( \|\mathbf{D} \cdot\|_1 , \mathbf{x} \right )$ --- whenever $\mathbf{x}$ is a piecewise-constant graph signal --- can lead to a high-probability lower bound on the minimum $q$-gain functional. Such a result would be more powerful than the certificate from Chapter \ref{ch:inexact_dual}, because it would imply robustness guarantees for the noisy regression problem \eqref{eq:l1_regression}. In fact, it would even be useful in more general settings where the measurements undergo some non-linear transformation before becoming available~\cite{plan2016}.
    \item \textbf{Practical sampling designs.} The optimization program in Corollary \ref{cor:opt_samp_design} is explicit but not practical a priori. I used a simplified version of it in Chapter \ref{ch:numerical_tour} but even then the sampling design that performed well was the one that demanded knowing the signal-to-be-sampled before even sampling it. We are thus left with a modelling problem. The optimal design depends on the action of the projection operator $\mathbf{P}_{\mathcal{S}}$, but knowing this operator is tantamount to knowing the jump-set $\mathcal{S}$ of the signal we want to recover. How much is it possible to assume, in practice, about the actions of $\mathbf{P}_{\mathcal{S}}$ without explicitly knowing $\mathcal{S}$?
\end{itemize}
